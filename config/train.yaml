defaults:
  - _self_
  - lr_scheduler: ReduceLROnPlateau
  - override hydra/job_logging: none

# Prevent Hydra from creating new directories; Lightning handles this already.
hydra:
  output_subdir: null
  run:
    dir: .

# Enable this to run a single batch of training for quick debugging
fast_dev_run: false

# Displayed in W&B, and used for the logs folder name.
project_name: 'vlr-project'

exp_group: null

log:
  save_dir: '/data/user_data/mprabhud/shantanu/vlr_project_logs2'
  freq: 50  # Log to W&B every X steps
  run_name: 'debug'

# Set this to the path of the checkpoint you want to resume from (if any).
# e.g. 'logs/vlr-project/<run_id>/checkpoints/last.ckpt'
#resume_from_ckpt: 'logs/vlr-project/xcm1bjxg/checkpoint/last.ckpt'
resume_from_ckpt: null


# ImageNet dataset folder structure: imagenet_raw/train/<subdir>/<image>.JPEG
data_dir: '/data/user_data/mprabhud/shantanu/datasets/train_0'
# The image_paths_pkl file contains the sub-paths to each image in data_dir.
# e.g. 'n01440764/n01440764_10026.JPEG'
#image_paths_pkl: './imagenet_train_paths.pkl'
image_paths_pkl: null


# Will resize & crop every image to this size, then divide into patches.
resize_image_to_px: 512
crop_image_to_px: 512
patch_size: 16 #8,16,32,64 
num_patches: -1 # This is calculated at runtime using crop_image_to_px and patch_size.

# ViT hyperparameters
patch_emb_dim: 1024
num_layers: 6 #4,6,8,10   
num_heads: 16
head_dim: 64


# We use a (fixed) random subset of the ImageNet dataset for training.
rand_subset_size: -1

train_percent: 0.8
val_percent: 0.2
num_workers: 4

num_epochs: 500
batch_size: 32
learning_rate: 1e-4

early_stop:
  _target_: 'lightning.pytorch.callbacks.EarlyStopping'
  monitor: 'val/t1_acc'
  mode: 'max'
  patience: 10
  min_delta: 0.001
  verbose: true

save_best_ckpt:
  _target_: 'lightning.pytorch.callbacks.ModelCheckpoint'
  monitor: 'val/t1_acc'
  mode: 'max'
  save_top_k: 1
  filename: 'best-{epoch}'
  save_last: true
  save_weights_only: false

torch_rng_seed: 42
numpy_rng_seed: 43
python_rng_seed: 44
